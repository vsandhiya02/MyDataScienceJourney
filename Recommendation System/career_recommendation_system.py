# -*- coding: utf-8 -*-
"""career recommended system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FLs_MzU-9rSO6536Ekn5B-rqvdYJKSHF
"""

import pandas as pd

import nltk
nltk.download('stopwords')
nltk.download('punkt')
import nltk
nltk.download('punkt_tab')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Load dataset
df = pd.read_csv('career.csv')

df

data_combine = df[['Career_Title', 'Required_Skills', 'Related_Interests']]

df['data_combine'] = df.apply(lambda x: ' - '.join(x), axis = 1)

df

data_combine.head(2)

df['data_combine'].head(1)[0]

"""**Pre - Processing**"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install contractions

import re
from bs4 import BeautifulSoup
import unicodedata
import contractions
import spacy
import nltk

nlp = spacy.load('en_core_web_sm')
ps = nltk.porter.PorterStemmer()


def strip_html_tags(text):
    soup = BeautifulSoup(text, "html.parser")
    [s.extract() for s in soup(['iframe', 'script'])]
    stripped_text = soup.get_text()
    stripped_text = re.sub(r'[\r|\n|\r\n]+', '\n', stripped_text)
    return stripped_text


def remove_accented_chars(text):
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    return text


def expand_contractions(text):
    return contractions.fix(text)


def spacy_lemmatize_text(text):
    text = nlp(text)
    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
    return text


def simple_stemming(text, stemmer=ps):
    text = ' '.join([stemmer.stem(word) for word in text.split()])
    return text


def remove_special_characters(text, remove_digits=False):
    pattern = r'[A-Z^a-z0-9\s]' if not remove_digits else r'[^a-zA-Z\s]'
    text = re.sub(pattern, '', text)
    return text


def remove_stopwords(text, is_lower_case=False, stopwords=None):
    if not stopwords:
        stopwords = nltk.corpus.stopwords.words('english')
    tokens = nltk.word_tokenize(text)
    tokens = [token.strip() for token in tokens]

    if is_lower_case:
        filtered_tokens = [token for token in tokens if token not in stopwords]
    else:
        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]

    filtered_text = ' '.join(filtered_tokens)
    return filtered_text

import tqdm

def text_pre_processor(text, html_strip=True, accented_char_removal=True, contraction_expansion=True,
                       text_lower_case=True, text_stemming=False, text_lemmatization=True,
                       special_char_removal=True, remove_digits=True, stopword_removal=True,
                       stopword_list=None):

    # strip HTML
    if html_strip:
        text = strip_html_tags(text)

    # remove extra newlines (often might be present in really noisy text)
    text = text.translate(text.maketrans("\n\t\r", "   "))

    # remove accented characters
    if accented_char_removal:
        text = remove_accented_chars(text)

    # expand contractions
    if contraction_expansion:
        text = expand_contractions(text)


    # lemmatize text
    if text_lemmatization:
        text = spacy_lemmatize_text(text)

    # remove special characters and\or digits
    if special_char_removal:
        # insert spaces between special characters to isolate them
        special_char_pattern = re.compile(r'([{.(-)!}])')
        text = special_char_pattern.sub(" \1 ", text)
        text = remove_special_characters(text, remove_digits=remove_digits)

    # stem text
    if text_stemming and not text_lemmatization:
        text = simple_stemming(text)

    # lowercase the text
    if text_lower_case:
        text = text.lower()


    # remove stopwords
    if stopword_removal:
        text = remove_stopwords(text, is_lower_case=text_lower_case,
                                stopwords=stopword_list)

    # remove extra whitespace
    text = re.sub(' +', ' ', text)
    text = text.strip()

    return text

df['career_info'] = df['data_combine'].apply(lambda x : text_pre_processor(x))

df

"""**Text To Number**"""

tf_idf = TfidfVectorizer(ngram_range = (1,2))

tf_idf_matrix = tf_idf.fit_transform(df['career_info'])

tf_idf_matrix.shape

"""**Cosine Similarity**"""

cosine_similarity(tf_idf_matrix).shape

doc_similarity = pd.DataFrame(cosine_similarity(tf_idf_matrix), columns= df['Career_Title'], index = df['Career_Title'])

doc_similarity

import numpy as np
np.argsort(-(doc_similarity['Data Analyst'].values))[:10]

career = df['Career_Title'].values

career[[60, 31, 59, 58, 24]]

def Career_Recommendation(career_title, career,doc_similarity):
  similarity_index = np.argsort(-(doc_similarity[career_title].values))[:5]
  return career[similarity_index][1:5]

# Example: Get the top 5 recommended careers for 'Data Analyst'
career_title_to_recommend = 'Data Analyst'
similarity_scores = doc_similarity[career_title_to_recommend]

# Sort the scores in descending order and get the indices of the top careers
sorted_indices = np.argsort(-similarity_scores)

# Get the top 5 career titles based on the sorted indices (excluding the input career itself)
top_careers = career[sorted_indices][1:6]

print(f"Top 5 recommended careers for '{career_title_to_recommend}':")
print(top_careers)

# Ask the user for a career title
career_title_to_recommend = input("Enter a career title to get recommendations: ")

# Check if the career exists in the similarity matrix
if career_title_to_recommend not in doc_similarity.columns:
    print(f"'{career_title_to_recommend}' not found in the dataset. Please try another career.")
else:
    # Get similarity scores for the input career
    similarity_scores = doc_similarity[career_title_to_recommend]

    # Sort scores in descending order
    sorted_indices = np.argsort(-similarity_scores)

    # Get top 5 similar careers, excluding the input itself
    top_careers = career[sorted_indices][1:6]

    print(f"\nTop 5 recommended careers for '{career_title_to_recommend}':")
    for i, title in enumerate(top_careers, 1):
        print(f"{i}. {title}")

"""**Train the word embeddings - FastText**

"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install gensim

from gensim.models import FastText

tokenized_doc_data = [doc.split() for doc in df['career_info']]

model = FastText(
    sentences=tokenized_doc_data,
    vector_size=100,     # dimensionality of word vectors
    window=5,            # context window size
    min_count=1,         # ignore words with total frequency lower than this
    workers=4,           # number of worker threads
    sg=0,                # 1 = skip-gram; 0 = CBOW
    epochs=100,            # number of training epochs
)

model.wv.most_similar('Writer')

model.wv['code'].size

i = 1
for sen in tokenized_doc_data[:10]:
  print("Document "+str(i)+" has "+str(len(sen))+" words ")
  i+=1

feature_vector = np.zeros(100,)
nwords = 0
vocabulary = set(model.wv.index_to_key)
for word in tokenized_doc_data[0]:
  if word in vocabulary:
    nwords+=1
  print("Word : ",word ,model.wv[word])
  #doc 0 => 24*100 => 100

  feature_vector = np.add(feature_vector,model.wv[word])


print(len(feature_vector))
print(nwords)
np.divide(feature_vector,nwords)

def document_to_vector(documents, model):
    vector_size = model.wv.vector_size
    vocabulary = set(model.wv.index_to_key)
    document_vectors = []
    for document in documents:
        feature_vector = np.zeros(vector_size,)
        nwords = 0
        for word in document:
            if word in vocabulary:
                nwords += 1
                feature_vector = np.add(feature_vector, model.wv[word])
        if nwords > 0:
            feature_vector = np.divide(feature_vector, nwords)
        document_vectors.append(feature_vector)
    return np.array(document_vectors)

document_vectors_array = document_to_vector(tokenized_doc_data, model)

len(document_vectors_array)

len(document_vectors_array[0])

len(document_vectors_array[35])

doc_sim = cosine_similarity(document_vectors_array)

doc_similarity = pd.DataFrame(cosine_similarity(document_vectors_array), columns= df['Career_Title'], index = df['Career_Title'])

doc_similarity

# Ask the user for a career title
career_title_to_recommend = input("Enter a career title to get recommendations: ")

# Check if the career exists in the similarity matrix
if career_title_to_recommend not in doc_similarity.columns:
    print(f"'{career_title_to_recommend}' not found in the dataset. Please try another career.")
else:
    # Get similarity scores for the input career
    similarity_scores = doc_similarity[career_title_to_recommend]

    # Sort scores in descending order
    sorted_indices = np.argsort(-similarity_scores)

    # Get top 5 similar careers, excluding the input itself
    top_careers = career[sorted_indices][1:6]

    print(f"\nTop 5 recommended careers for '{career_title_to_recommend}':")
    for i, title in enumerate(top_careers, 1):
        print(f"{i}. {title}")